# BAML Source Files

This directory contains the BAML prompt definitions that generate the LLM code and response functions for the workflow agent.

## What is BAML?

[BAML](https://github.com/BinderAI/baml) (Binder AI Language) is a prompt engineering tool that:
- Lets you write prompts in a dedicated DSL (Domain Specific Language)
- Generates type-safe Python client code via `baml generate`
- Separates prompt logic from application code

## Project Structure

```
baml_src/
├── readme.md              # This file
├── clients.baml           # LLM client configurations (OpenRouter, etc.)
├── generators.baml        # Generator settings (temperature, model, etc.)
└── workflow_agent.baml    # Main prompts for planning, chat, codegen, respond
```

### Key Files

| File | Purpose |
|------|---------|
| `workflow_agent.baml` | Contains all 6 prompt functions: `WorkflowPlan`, `WorkflowPlanReview`, `WorkflowCodegen`, `WorkflowChat`, `WorkflowRespond` |
| `clients.baml` | Defines LLM clients (e.g., `OpenRouterChat`) used by prompts |
| `generators.baml` | Sets generation parameters like temperature, max tokens |

## Workflow Functions

The agent uses 5 main BAML functions (defined in `workflow_agent.baml`):

| Function | Input | Output | When Used |
|----------|-------|--------|-----------|
| `WorkflowPlan` | user_message, skills_readme, skill_names, skill_groups | `Plan` (action, intent, steps) | First step: classify request as chat/skill/custom |
| `WorkflowPlanReview` | user_message, proposed_plan, skill_md | `Plan` (reviewed) | HITL: review user feedback on plan |
| `WorkflowCodegen` | user_message, plan_json, skill_md, tool_contracts, attempt | Python code string | Execute: generate script to run |
| `WorkflowChat` | user_message, skills_readme, custom_skill_md | string | When action="chat": conversational response |
| `WorkflowRespond` | user_message, plan_json, exec_stdout, exit_code, attempts | string | After execution: summarize results |

## The Plan Schema

All planning functions return a `Plan` object:

```baml
class Plan {
  action PlanAction       // Chat | ExecuteSkill | CustomScript
  skill_group string?     // HR-scopes, Recruitment-scopes, etc.
  skill_name string?      // Specific skill (e.g., "onboard_new_hire")
  intent string           // Human-readable description
  steps string[]          // High-level steps (for execution)
}
```

## Making Changes

### 1. Edit the BAML source

Modify prompts in `workflow_agent.baml`. Key sections:
- `prompt #"..."#` - The actual prompt text
- `client OpenRouterChat` - Which LLM to use
- Input variables like `{{ user_message }}` - Dynamic content

### 2. Regenerate the client

```bash
cd /path/to/repo
baml generate
```

This updates `baml_client/` with new Python functions.

### 3. Verify

```bash
python -m pytest -q
```

### Example: Adding a New Prompt

1. Add to `workflow_agent.baml`:
```baml
function MyNewFunction(input: string) -> string {
  client OpenRouterChat
  prompt #"
  Your prompt here: {{ input }}
  "#
}
```

2. Run `baml generate`

3. Call from Python:
```python
from agent_workspace.workflow_agent.baml_bridge import workflow_my_new_function
result = workflow_my_new_function(input="hello")
```

## Prompt Engineering Tips

### Accessing Variables
- `{{ variable }}` - Inserts variable value
- `{% for item in list %}` ... `{% endfor %}` - Loops
- `{% if condition %}` ... `{% endif %}` - Conditionals

### Output Format
Use `{{ ctx.output_format }}` at the end of prompts to inject the required JSON schema.

### Best Practices
1. Keep prompts concise - LLMs obey explicit constraints
2. Use "IMPORTANT:" prefixes for critical rules
3. Test edge cases (empty inputs, unusual requests)
4. After changes, run `baml generate` before testing

## Debugging Prompts

1. **Check generated client**: Review `baml_client/__init__.py` to verify function signatures
2. **Print prompts**: Add `print(f"Prompt: {prompt}")` in `baml_bridge.py` for debugging
3. **Check LLM responses**: The raw output from OpenRouter may reveal issues

## Related Files

- [agent_workspace/workflow_agent/agent.py](../agent_workspace/workflow_agent/agent.py) - Agent that calls BAML functions
- [agent_workspace/workflow_agent/baml_bridge.py](../agent_workspace/workflow_agent/baml_bridge.py) - Wrapper functions
- [baml_client/](../baml_client/) - Generated Python client (DO NOT EDIT - regenerated by `baml generate`)
